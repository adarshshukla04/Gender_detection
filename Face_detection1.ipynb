{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985821b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization as BN, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7ae7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda\\envs\\tensorflowenv\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-0.24.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ff3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70850a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96276a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d9b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization as BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413560a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array that will store data and its labels\n",
    "data_train   = []\n",
    "labels_train = []\n",
    "data_test    = []\n",
    "labels_test  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f12b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension of image limit\n",
    "img_dims = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a021ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image files from dataset\n",
    "image_files_train = [f for f in glob.glob(r'D:\\Face_detection\\Training' + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n",
    "image_files_test = [f for f in glob.glob(r'D:\\Face_detection\\Validation' + \"/**/*\", recursive=True) if not os.path.isdir(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "507b801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(image_files_train)\n",
    "random.shuffle(image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae83aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting images to array and labelling the categories\n",
    "def convert(image_files):\n",
    "    data = []\n",
    "    for img in image_files:\n",
    "        image = cv2.imread(img)\n",
    "    \n",
    "        image = cv2.resize(image, (img_dims[0], img_dims[1]))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "    return data\n",
    "#Saving label of image\n",
    "def label(image_files):\n",
    "    labels = []\n",
    "    for img in image_files:\n",
    "        label = img.split(os.path.sep)[-2] #D:\\\\Face_detection\\\\Training\\\\male\\\\135791.jpg.jpg\n",
    "        if label == \"female\":\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        labels.append([label]) #[[0], [1], [1], ....]\n",
    "    return labels;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57fc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = convert(image_files_train)\n",
    "data_test  = convert(image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74894fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = label(image_files_train)\n",
    "labels_test  = label(image_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84b39838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "data_train   = np.array(data_train, dtype=\"float\")\n",
    "labels_train = np.array(labels_train)\n",
    "data_test    = np.array(data_test, dtype=\"float\")\n",
    "labels_test  = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "248b36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(labels_train, num_classes = 2)\n",
    "testY  = to_categorical(labels_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c0c3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = data_train\n",
    "testX  = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71ca6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation dataset\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dc4d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth) #(96, 96, 3) -> in our case\n",
    "    chanDim = -1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channel_first' or channels_last\n",
    "        imputShape = (depth, height, width)\n",
    "        chanDim = 1\n",
    "        \n",
    "    model.add(Conv2D(32, (3,3), padding = \"Same\", input_shape = inputShape))\n",
    "    \n",
    "    model.add(Activation(\"relu\")) #as we are dealing with non-linear dataset\n",
    "    \n",
    "    model.add(BN(axis = chanDim))#It is possible that higer data value may overshadow the lower value hence normalisation is done.\n",
    "    #BatchNormalisation is used to keep meanActivation close to '0' and standard deviation close to '1'.\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))#maxpooling is used to reduce noice in dataset like: spots on face, beard, pimples, etc.\n",
    "    #It looks only important features (eyes, noes, etc.) and remove others.\n",
    "    \n",
    "    model.add(Dropout(0.25)) # To avoid overfitting\n",
    "    #25% neurons will be deactivated during front and backward propagation\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BN(axis = chanDim))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BN(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BN(axis = chanDim))\n",
    "    \n",
    "    model.add(Conv2D(64, (3,3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BN(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten()) #Flatten converts 2D into 1D\n",
    "    model.add(Dense(1024)) #Add dense layer with 1024 neurons\n",
    "    model.add(Activation(\"relu\"))# relu is go to activation function when we do image classification\n",
    "    model.add(BN())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\")) #in outer we have given sigmod as it is probability based function ad give value from 0 to 1\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96472ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2], classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e479c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "epochs = 70\n",
    "lr = 1e-3\n",
    "batch_size = 25\n",
    "\n",
    "opt = Adam(learning_rate = lr, decay = lr/epochs) #Adam is optimizer->during back propagation how the weight should be updated\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7b4e91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\gputest\\lib\\site-packages (1.5.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\anaconda\\envs\\gputest\\lib\\site-packages (from scipy) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "208907b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a673a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19440 samples\n",
      "Epoch 1/70\n",
      "19440/19440 [==============================] - 41s 2ms/sample - loss: 0.3073 - accuracy: 0.8800\n",
      "Epoch 2/70\n",
      "19440/19440 [==============================] - 31s 2ms/sample - loss: 0.1951 - accuracy: 0.9243\n",
      "Epoch 3/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1689 - accuracy: 0.9352\n",
      "Epoch 4/70\n",
      "19440/19440 [==============================] - 31s 2ms/sample - loss: 0.1642 - accuracy: 0.9402\n",
      "Epoch 5/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1659 - accuracy: 0.9376\n",
      "Epoch 6/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1382 - accuracy: 0.9479\n",
      "Epoch 7/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1283 - accuracy: 0.9527\n",
      "Epoch 8/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1173 - accuracy: 0.9569\n",
      "Epoch 9/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1126 - accuracy: 0.9592\n",
      "Epoch 10/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1246 - accuracy: 0.9555\n",
      "Epoch 11/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0972 - accuracy: 0.9642\n",
      "Epoch 12/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0938 - accuracy: 0.9658\n",
      "Epoch 13/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1022 - accuracy: 0.9611\n",
      "Epoch 14/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.1106 - accuracy: 0.9591\n",
      "Epoch 15/70\n",
      "19440/19440 [==============================] - 31s 2ms/sample - loss: 0.0836 - accuracy: 0.9690\n",
      "Epoch 16/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0916 - accuracy: 0.9669\n",
      "Epoch 17/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0982 - accuracy: 0.9649\n",
      "Epoch 18/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0776 - accuracy: 0.9722\n",
      "Epoch 19/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0923 - accuracy: 0.9648\n",
      "Epoch 20/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0687 - accuracy: 0.9751\n",
      "Epoch 21/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0663 - accuracy: 0.9759\n",
      "Epoch 22/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0723 - accuracy: 0.9721\n",
      "Epoch 23/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0574 - accuracy: 0.9788\n",
      "Epoch 24/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0716 - accuracy: 0.9740\n",
      "Epoch 25/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0562 - accuracy: 0.9801\n",
      "Epoch 26/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0546 - accuracy: 0.9802\n",
      "Epoch 27/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0574 - accuracy: 0.9788\n",
      "Epoch 28/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0472 - accuracy: 0.9829\n",
      "Epoch 29/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0422 - accuracy: 0.9841\n",
      "Epoch 30/70\n",
      "19440/19440 [==============================] - 31s 2ms/sample - loss: 0.0479 - accuracy: 0.9825\n",
      "Epoch 31/70\n",
      "19440/19440 [==============================] - 35s 2ms/sample - loss: 0.0452 - accuracy: 0.9842\n",
      "Epoch 32/70\n",
      "19440/19440 [==============================] - 41s 2ms/sample - loss: 0.0422 - accuracy: 0.9851\n",
      "Epoch 33/70\n",
      "19440/19440 [==============================] - 41s 2ms/sample - loss: 0.0456 - accuracy: 0.9831\n",
      "Epoch 34/70\n",
      "19440/19440 [==============================] - 39s 2ms/sample - loss: 0.0489 - accuracy: 0.9824\n",
      "Epoch 35/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0344 - accuracy: 0.9880\n",
      "Epoch 36/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0329 - accuracy: 0.9885\n",
      "Epoch 37/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0361 - accuracy: 0.9873\n",
      "Epoch 38/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0297 - accuracy: 0.9895\n",
      "Epoch 39/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0375 - accuracy: 0.9870\n",
      "Epoch 40/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0296 - accuracy: 0.9892\n",
      "Epoch 41/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0338 - accuracy: 0.9875\n",
      "Epoch 42/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 43/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0343 - accuracy: 0.9882\n",
      "Epoch 44/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0323 - accuracy: 0.9879\n",
      "Epoch 45/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0316 - accuracy: 0.9889\n",
      "Epoch 46/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0324 - accuracy: 0.9881\n",
      "Epoch 47/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0273 - accuracy: 0.9903\n",
      "Epoch 48/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0241 - accuracy: 0.9915\n",
      "Epoch 49/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0305 - accuracy: 0.9896\n",
      "Epoch 50/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0379 - accuracy: 0.9863\n",
      "Epoch 51/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0284 - accuracy: 0.9899\n",
      "Epoch 52/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0224 - accuracy: 0.9924\n",
      "Epoch 53/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0200 - accuracy: 0.9930\n",
      "Epoch 54/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0247 - accuracy: 0.9913\n",
      "Epoch 55/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0379 - accuracy: 0.9867\n",
      "Epoch 56/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0447 - accuracy: 0.9842\n",
      "Epoch 57/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0329 - accuracy: 0.9886\n",
      "Epoch 58/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0326 - accuracy: 0.9885\n",
      "Epoch 59/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0264 - accuracy: 0.9908\n",
      "Epoch 60/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0190 - accuracy: 0.9933\n",
      "Epoch 61/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0292 - accuracy: 0.9899\n",
      "Epoch 62/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0262 - accuracy: 0.9905\n",
      "Epoch 63/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0228 - accuracy: 0.9923\n",
      "Epoch 64/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0212 - accuracy: 0.9925\n",
      "Epoch 65/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0259 - accuracy: 0.9900\n",
      "Epoch 66/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0225 - accuracy: 0.9925\n",
      "Epoch 67/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0180 - accuracy: 0.9936\n",
      "Epoch 68/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0175 - accuracy: 0.9936\n",
      "Epoch 69/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0177 - accuracy: 0.9936\n",
      "Epoch 70/70\n",
      "19440/19440 [==============================] - 30s 2ms/sample - loss: 0.0167 - accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "'''H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX,testY),\n",
    "              steps_per_epoch = len(trainX)//batch_size,\n",
    "              epochs = epochs, verbose =1)'''\n",
    "H = model.fit(trainX, trainY, batch_size=batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f97df430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: face_recognition.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"face_recognition.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e492a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
